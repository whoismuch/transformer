{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T10:17:57.653619Z",
     "start_time": "2024-01-20T10:17:57.639553Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# один блок трансформера\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # слой многоголового внимания, который использует входные данные для вычисления взвешенной суммы \n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # feed forward сеть, состоит из 2 полносвязных слоев с фцией активации relu (позволяет модели извлекать более сложные и нелинейные зависимости в данных, дополняя результаты многоголового внимания)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        # слои нормализации (для стабилизации и нормализации выходов блока)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # два дропаут слоя (случайное \"выключение\" части нейронов во время обучения)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # проход через многоголовое внимание\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        # дропаутим\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # нормализуем\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        # фидворвард сеть\n",
    "        ffn_output = self.ffn(out1)\n",
    "        # еще дропаутим\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # еще немного нормализуем \n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T11:28:20.717162Z",
     "start_time": "2024-01-20T11:28:20.708500Z"
    }
   },
   "id": "e64b9a15d0533408"
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        # эмбеддинги для токенов\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        # эмбеддинги для позиции\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    # как слой будет применяться к входным данным во время прямого прохода\n",
    "    def call(self, x):\n",
    "        # определяем макс длинну последовательности из x\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        # создаем вектор с позициями (последовательность чисел от 0 до maxlen - 1 с шагом 1)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # применяем этот слой к вектору позиций, чтобы получить эмбеддинги для каждой позиции в последовательности \n",
    "        # -> каждая позиция теперь представлена в виде вектора\n",
    "        positions = self.pos_emb(positions)\n",
    "        # применяем эмбеддинги слов к входным данным \n",
    "        # -> каждая слово в последовательности теперь представлено в виде вектора\n",
    "        x = self.token_emb(x)\n",
    "        \n",
    "        return x + positions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T11:28:21.602134Z",
     "start_time": "2024-01-20T11:28:21.545971Z"
    }
   },
   "id": "fc18a24fdbb8abfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "200a4adba2627965"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7206 Training sequences\n",
      "7206 Validation sequences\n",
      "[0. 1. 1. ... 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataloader\n",
    "\n",
    "importlib.reload(dataloader)\n",
    "\n",
    "maxlen = 50\n",
    "num_words = 20000\n",
    "\n",
    "# загружаем данные из нашего датасета (уже с учетом токенизации и паддингов)\n",
    "(x_train, y_train), (x_val, y_val) = dataloader.load_data(num_words=num_words, maxlen = maxlen)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "print(y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T10:18:00.471844Z",
     "start_time": "2024-01-20T10:17:59.845046Z"
    }
   },
   "id": "733a10c896fb6bde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67daafd06d3b879"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "embed_dim = 32  # Размерность эмбеддинга для каждого токена\n",
    "num_heads = 2  # Количество голов внимания\n",
    "ff_dim = 32  # Размер скрытого слоя в сети прямого распространения внутри трансформера.\n",
    "\n",
    "# создаем входной слой размера maxlen\n",
    "inputs = Input(shape=(maxlen,))\n",
    "# инициализируем слой эмбеддинга, который включает в себя как эмбеддинги слов, так и эмбеддинги позиций.\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, num_words, embed_dim)\n",
    "# входные данные подаются на слой эмбеддинга, который возвращает эмбеддинги для каждого токена с учетом их позиций.\n",
    "x = embedding_layer(inputs)\n",
    "# создаем блок трансформера\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "# эмбеддинги подвергаются обработке блоком трансформера\n",
    "x = transformer_block(x)\n",
    "# глобальное усреднение и применение пулинга:\n",
    "# для уменьшения размерности перед последним слоем классификации. Этот слой выполняет глобальное усреднение значений по всей длине последовательности\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "#Применяем dropout и полносвязные слои\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "# создаем выходной слой\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# создаем модель\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T11:28:50.169734Z",
     "start_time": "2024-01-20T11:28:50.095911Z"
    }
   },
   "id": "2f512f8df6c180ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9820cbecdea61de4"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "181/181 [==============================] - 5s 23ms/step - loss: 0.5779 - accuracy: 0.6988 - val_loss: 0.3807 - val_accuracy: 0.8432\n",
      "Epoch 2/3\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.2648 - accuracy: 0.8929 - val_loss: 0.4248 - val_accuracy: 0.8193\n",
      "Epoch 3/3\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.1346 - accuracy: 0.9503 - val_loss: 0.3923 - val_accuracy: 0.8651\n"
     ]
    }
   ],
   "source": [
    "# компилируем нейронную сеть\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# обучаем\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=64, epochs=3, \n",
    "                    validation_data=(x_val, y_val)\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T10:15:44.758573Z",
     "start_time": "2024-01-20T10:15:31.248999Z"
    }
   },
   "id": "ec2d533672fd69d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "181f594ba42093eb"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 - 1s - loss: 0.3923 - accuracy: 0.8651 - 513ms/epoch - 6ms/step\n",
      "loss: 0.392\n",
      "accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "# Получаем метрики\n",
    "results = model.evaluate(x_val, y_val, verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(\"%s: %.3f\" % (name, value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T10:15:50.164123Z",
     "start_time": "2024-01-20T10:15:49.626582Z"
    }
   },
   "id": "868881b0a2387be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "531dba766a8d4568"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
